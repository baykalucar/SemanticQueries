{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and AI Democratization through PromptToQueryResult Function\n",
    "\n",
    "In the context of data and AI democratization, `PromptToQueryResult` could be a function that translates natural language prompts into database queries. This could be part of a larger system that allows non-technical users to interact with databases using natural language, which would democratize access to data and AI.\n",
    "\n",
    "## Business Value and Domain Problem Solutions\n",
    "\n",
    "### Business Value:\n",
    "\n",
    "- **Increased Efficiency:** Non-technical users can retrieve data without needing to learn SQL or other query languages, or without needing to wait for technical staff to retrieve the data for them.\n",
    "- **Reduced Costs:** Less time and resources are spent on training staff to use complex database systems, and less time is spent on data retrieval tasks by technical staff.\n",
    "- **Improved Decision Making:** With easier access to data, decision-making can be data-driven and timely, leading to better business outcomes.\n",
    "\n",
    "### Domain Problem Solved:\n",
    "\n",
    "- **Data Accessibility:** One of the challenges in data democratization is making data accessible to non-technical users. `PromptToQueryResult` could help solve this problem by providing a natural language interface to databases.\n",
    "- **AI Democratization:** By using AI to translate natural language prompts into database queries, `PromptToQueryResult` could also contribute to AI democratization. It could be part of a system that allows users to leverage the power of AI without needing to understand the technical details.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "The selected code is written in Python and it's part of a larger program that uses a plugin-based architecture. The code is using a kernel object to manage plugins and their functions.\n",
    "\n",
    "- **Variable Definitions:** First, two variables are defined: `plugins_directory` and `file_path`. The `plugins_directory` variable is set to the string `\"plugins\"`, which is presumably the directory where the plugins are stored. The `file_path` variable is set to `\"data_schema.txt\"`, which is likely a file that contains a data schema.\n",
    "\n",
    "- **Reading Data Schema:** The `read_data_schema_from_file` function is then called with `file_path` as an argument. This function reads the data schema from the specified file and returns it as a string. The returned data schema is stored in the `data_schema` variable.\n",
    "\n",
    "- **Conditional Plugin Import:** Next, there's an if-statement that checks the `prompt_rephrase` variable. If `prompt_rephrase` is true, the code imports a plugin named `\"PromptPlugin\"` from the plugins directory using the `import_plugin_from_prompt_directory` method of the kernel object. This method returns a dictionary-like object of functions provided by the plugin. The `\"PromptRephraser\"` function from the `\"PromptPlugin\"` plugin is then stored in the `rephraserFunction` variable.\n",
    "\n",
    "- **Function Invocation:** The `rephraserFunction` is then invoked asynchronously using the `invoke` method of the kernel object. The `invoke` method is called with two arguments: the function to be invoked and a `KernelArguments` object that contains the data schema and a query. The result of the function invocation is stored in the `rephrased_prompt` variable.\n",
    "\n",
    "- **Importing Another Plugin:** Finally, the code imports another plugin named `\"DataPlugin\"` from the plugins directory and stores the `\"DatabaseDescriptor\"` function from this plugin in the `descriptorFunction` variable. This function can presumably be used later in the code to describe a database based on the data schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from services import Service\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import dotenv_values\n",
    "import semantic_kernel.connectors.ai.hugging_face as sk_hf\n",
    "import datetime\n",
    "import json\n",
    "from huggingface_hub import InferenceApi\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from pydantic import BaseModel  # Import Pydantic BaseModel\n",
    "import requests  # Import requests library\n",
    "\n",
    "async def GenerateQuestions(selectedService=Service.AzureOpenAI):\n",
    "    \"\"\"\n",
    "    Generates possible questions using the Semantic Kernel and prints the results.\n",
    "    \"\"\"\n",
    "    kernel = sk.Kernel()\n",
    "    service_id = None\n",
    "    if selectedService == Service.OpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "        api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "        service_id = \"gpt4-32k\"\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(service_id=service_id, ai_model_id=\"gpt4-32k\", api_key=api_key, org_id=org_id),\n",
    "        )\n",
    "    elif selectedService == Service.AzureOpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "        deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "        service_id = \"aoai_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key),\n",
    "        )\n",
    "    elif selectedService == Service.HuggingFace:\n",
    "        from huggingface_hub import InferenceApi\n",
    "    \n",
    "        api_token = \"your_huggingface_api_token\"  # Set your HuggingFace API token here\n",
    "        model_id = \"huggingface-model-id\"  # Set the model ID you want to use from HuggingFace\n",
    "        \n",
    "        hf_service = InferenceApi(repo_id=model_id, token=api_token)\n",
    "        service_id = \"hf_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            HuggingFaceChatCompletion(service_id=service_id, hf_service=hf_service),\n",
    "        )\n",
    "    \n",
    "    plugins_directory = \"plugins\"\n",
    "    file_path = \"data_schema.txt\"\n",
    "    data_schema = read_data_schema_from_file(file_path)\n",
    "    promptFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"DataPlugin\")\n",
    "    queryGeneratorFunction = promptFunctions[\"QuestionGenerator\"]\n",
    "    result = await kernel.invoke(queryGeneratorFunction, sk.KernelArguments(data_schema=data_schema))\n",
    "    result_string = result.data if hasattr(result, 'data') else str(result)\n",
    "    # Generate a unique filename based on the current date\n",
    "    filename = \"questions/\" + datetime.datetime.now().strftime(\"%Y-%m-%d\") + \".txt\"\n",
    "\n",
    "    # Write result_string to the file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(result_string)\n",
    "\n",
    "    # Print the filename\n",
    "    print(\"Result saved to:\", filename)\n",
    "    print(result_string)\n",
    "\n",
    "async def ReadQuestionsAndGenerateAnswers(filename, debug=False, selectedService=Service.AzureOpenAI):\n",
    "    \"\"\"\n",
    "    Reads questions from a file and generates answers using the Semantic Kernel.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file containing the questions.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Extract the filename from the full path\n",
    "    filename = os.path.basename(filename)\n",
    "\n",
    "    # Remove the file extension\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Create the directory path\n",
    "    directory_path = \"answers/\" + filename \n",
    "\n",
    "    print(\"Directory path: \", directory_path)   \n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "\n",
    "    data = json.loads(content)\n",
    "    i = 0\n",
    "    for item in data:\n",
    "        complexity = item[\"complexity\"]\n",
    "        queries = item[\"queries\"]\n",
    "        \n",
    "        for question in queries:\n",
    "            print(\"Index:\" , i)\n",
    "            print(\"Question: \", question)\n",
    "            print(\"Complexity: \", complexity)\n",
    "            questionFolderName = i.__str__() + \"_\" + complexity\n",
    "            if not os.path.exists(directory_path + \"/\"  + questionFolderName + \"/\"):\n",
    "                os.makedirs(directory_path + \"/\"  + questionFolderName + \"/\")\n",
    "\n",
    "            try:\n",
    "                await PromptToQueryResult(debug=debug, prompt_rephrase=False, selectedService=selectedService, query=question, outputFileDir=directory_path + \"/\" + questionFolderName + \"/\")\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                with open(directory_path + \"/\" + questionFolderName + \"/error.txt\", \"w\") as file:\n",
    "                    file.write(error_message)\n",
    "            i = i + 1\n",
    "\n",
    "def make_safe_folder_name(folder_name):\n",
    "    # Define a regular expression pattern to match invalid characters\n",
    "    # This example replaces anything that is not a letter, number, underscore, or hyphen with an underscore\n",
    "    safe_folder_name = re.sub(r'[^A-Za-z0-9_\\-]', '_', folder_name)\n",
    "    return safe_folder_name\n",
    "\n",
    "async def PromptToQueryResult(debug=False, prompt_rephrase=False, selectedService=Service.AzureOpenAI, query=None, outputFileDir=\"\"):\n",
    "    \"\"\"\n",
    "    Prompts the user for a query, rephrases the prompt if required, and executes the query using the Semantic Kernel.\n",
    "\n",
    "    Args:\n",
    "        debug (bool, optional): If True, prints debug information. Defaults to False.\n",
    "        prompt_rephrase (bool, optional): If True, rephrases the prompt using a rephraser plugin. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame or any: The result of the executed query, or 'any' if no query is executed.\n",
    "    \"\"\"\n",
    "    kernel = sk.Kernel()\n",
    "    service_id = None\n",
    "    if selectedService == Service.OpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "        api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "        service_id = \"gpt4-32k\"\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(service_id=service_id, ai_model_id=\"gpt4-32k\", api_key=api_key, org_id=org_id),\n",
    "        )\n",
    "    elif selectedService == Service.AzureOpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "        deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "        service_id = \"aoai_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key),\n",
    "        )\n",
    "    elif selectedService == Service.HuggingFace:\n",
    "        from huggingface_hub import InferenceApi\n",
    "        model_id, api_token, api_url = hugging_face_settings_from_dot_env_llama318BInstruct(\"Llama318BInstruct\")\n",
    "        # api_token = \"hf_...\"  # Set your HuggingFace API token here\n",
    "        # model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # Set the model ID you want to use from HuggingFace\n",
    "        # api_url = f\"https://api-inference.huggingface.co/models/{model_id}/v1/chat/completions\"\n",
    "        # Initialize the HuggingFace Inference API without task override\n",
    "        hf_service = InferenceApi(repo_id=model_id, token=api_token)\n",
    "    \n",
    "\n",
    "       # Define a simple completion object to hold both the response and metadata\n",
    "        class CompletionResult:\n",
    "            def __init__(self, content: str, metadata: dict = None):\n",
    "                self.content = content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "            def __iter__(self):\n",
    "                return iter([self.content])  # Ensures CompletionResult can be iterated over\n",
    "\n",
    "        # Create the HuggingFaceChatCompletion class with a custom request\n",
    "        class HuggingFaceChatCompletion(ChatCompletionClientBase, BaseModel):\n",
    "            service_id: str\n",
    "            api_token: str\n",
    "            api_url: str\n",
    "            \n",
    "            class Config:\n",
    "                arbitrary_types_allowed = True\n",
    "\n",
    "            async def complete_async(self, messages: list, max_tokens: int = 500, stream: bool = False, **kwargs):\n",
    "                # Create the headers and payload for the request\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {self.api_token}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                }\n",
    "\n",
    "                payload = {\n",
    "                    \"model\": self.api_url.split('/')[-4],  # model_id\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"stream\": stream\n",
    "                }\n",
    "\n",
    "                # Make the request to HuggingFace API\n",
    "                response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "                response_json = response.json()\n",
    "\n",
    "                # Log the response for debugging\n",
    "                # print(\"HuggingFace API Response:\", response_json)\n",
    "                \n",
    "                # Handle the response format\n",
    "                if \"choices\" in response_json:\n",
    "                    content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    # Return a list of CompletionResult objects, even if there's only one\n",
    "                    return [CompletionResult(content=content, metadata={\"model\": self.ai_model_id})]\n",
    "                    #return content\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected response format: {response_json}\")\n",
    "\n",
    "            # Updated complete_chat to handle chat_history and settings\n",
    "            async def complete_chat(self, chat_history, settings, **kwargs):\n",
    "                messages = [{\"role\": \"user\", \"content\": msg.content} for msg in chat_history]\n",
    "                return await self.complete_async(messages)\n",
    "            \n",
    "            # Implement required complete_chat_stream method\n",
    "            async def complete_chat_stream(self, chat_history, settings, **kwargs):\n",
    "                # For non-streaming models, we can return the entire result as one chunk\n",
    "                result = await self.complete_chat(chat_history, settings)\n",
    "                for chunk in [result]:  # Mock streaming by yielding the entire result as a single chunk\n",
    "                    yield chunk\n",
    "\n",
    "        service_id = \"hf_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            HuggingFaceChatCompletion(service_id=service_id, api_token=api_token, api_url=api_url, ai_model_id=model_id),\n",
    "        )\n",
    "\n",
    "    \n",
    "    if(query is None):\n",
    "        query = input(\"Enter your query: \")  # Get query from user\n",
    "    rephrased_prompt = query\n",
    "    \n",
    "    plugins_directory = \"plugins\"\n",
    "    file_path = \"data_schema.txt\"\n",
    "    data_schema = read_data_schema_from_file(file_path)\n",
    "    if(prompt_rephrase):\n",
    "        promptFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"PromptPlugin\")\n",
    "        rephraserFunction = promptFunctions[\"PromptRephraser\"]\n",
    "        rephrased_prompt_result = await kernel.invoke(rephraserFunction, sk.KernelArguments(data_schema=data_schema, query=query))\n",
    "        # Debug: Print the type and value of the result to understand its structure\n",
    "        # print(type(rephrased_prompt_result))  # This will show whether it's a tuple, list, or other type\n",
    "        # print(rephrased_prompt_result)        # This will print the raw result\n",
    "        # print(\"Kernel invoke result:\", rephrased_prompt_result.__dict__)\n",
    "        if(hasattr(rephrased_prompt_result, 'data')):\n",
    "            rephrased_prompt = rephrased_prompt_result.data\n",
    "        elif (rephrased_prompt_result.__dict__):\n",
    "            # Access the value (which contains the list of CompletionResult objects)\n",
    "            # print(\"Value: \", rephrased_prompt_result.value)\n",
    "            completion_results = rephrased_prompt_result.value\n",
    "\n",
    "            # Check if it's a list and access the content of the first CompletionResult\n",
    "            if isinstance(completion_results, list) and len(completion_results) > 0:\n",
    "                first_result = completion_results[0]\n",
    "                # print(first_result.content)  # This will print the content of the first result\n",
    "                rephrased_prompt = first_result.content\n",
    "            else:\n",
    "                print(\"No completion results found.\")\n",
    "        else:\n",
    "            rephrased_prompt = str(rephrased_prompt_result)\n",
    "        #rephrased_prompt = rephrased_prompt_result.data if hasattr(rephrased_prompt_result, 'data') else str(rephrased_prompt_result)\n",
    "\n",
    "    dataFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"DataPlugin\")\n",
    "    descriptorFunction = dataFunctions[\"DatabaseDescriptor\"]\n",
    "\n",
    "    savePlotToDisk = \"\"\n",
    "    if(outputFileDir != \"\"):\n",
    "        savePlotToDisk = \"Generated plots should be saved in the directory: \" + outputFileDir + \"plot.png\"\n",
    "\n",
    "    if(outputFileDir != \"\"):\n",
    "        # Write rephrased prompt to query.txt file\n",
    "        with open(outputFileDir + \"user_prompt.txt\", \"w\") as file:\n",
    "            file.write(rephrased_prompt)\n",
    "\n",
    "    result = await kernel.invoke(descriptorFunction, sk.KernelArguments(data_schema=data_schema, query= rephrased_prompt, save_plot_to_disk = savePlotToDisk))\n",
    "    if(hasattr(result, 'data')):\n",
    "        result_string = result.data\n",
    "    elif (result.__dict__):\n",
    "        # Access the value (which contains the list of CompletionResult objects)\n",
    "        # print(\"Value: \", result.value)\n",
    "        completion_results = result.value\n",
    "\n",
    "        # Check if it's a list and access the content of the first CompletionResult\n",
    "        if isinstance(completion_results, list) and len(completion_results) > 0:\n",
    "            first_result = completion_results[0]\n",
    "            # print(first_result.content)  # This will print the content of the first result\n",
    "            result_string = first_result.content\n",
    "        else:\n",
    "            print(\"No completion results found.\")\n",
    "    else:\n",
    "        result_string = str(result)\n",
    "    if(debug):\n",
    "        print(\"result String:\", result_string)\n",
    "    matches_sql = parse_text_between_tags(result_string,\"<sql>\", \"</sql>\")\n",
    "\n",
    "    if(prompt_rephrase):\n",
    "        print(\"User query: \" + query)\n",
    "    print(\"Rephrased prompt: \" + rephrased_prompt + \"#\")\n",
    "    if len(matches_sql) > 0:\n",
    "        sql = matches_sql[0]\n",
    "        if(outputFileDir != \"\"):\n",
    "            # Write query to .txt file\n",
    "            with open(outputFileDir + \"sql_query.txt\", \"w\") as file:\n",
    "                file.write(sql)\n",
    "        if debug:\n",
    "            print(\"SQL: \", sql)\n",
    "        df = run_sql_query(sql)\n",
    "    \n",
    "    \n",
    "\n",
    "    matches_python = parse_text_between_tags(result_string,\"<python>\", \"</python>\")\n",
    "    if len(matches_python) > 0:\n",
    "        if debug:\n",
    "            print(\"PYTHON:\", matches_python[0])\n",
    "        try:\n",
    "            db_conn = os.getenv(\"DB_CONNECTION_STRING\")\n",
    "            conn = sqlite3.connect(db_conn)\n",
    "            exec(matches_python[0])\n",
    "            conn.close()\n",
    "\n",
    "            if(outputFileDir != \"\"):\n",
    "                # Write python code to .txt file\n",
    "                with open(outputFileDir + \"python_code.txt\", \"w\") as file:\n",
    "                    file.write(matches_python[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print('hata:' + e)\n",
    "        except:\n",
    "            print(\"An exception occurred\")\n",
    "    if len(matches_sql) > 0:\n",
    "        df.head()\n",
    "        if(outputFileDir != \"\"):\n",
    "            df.to_csv(outputFileDir + \"output.csv\", index=False)\n",
    "        return df\n",
    "    else:  \n",
    "        return any\n",
    "\n",
    "def hugging_face_settings_from_dot_env_llama318BInstruct(p_model=\"Llama318BInstruct\") :\n",
    "\n",
    "    config = dotenv_values(\".env\")\n",
    "    model_name = config.get(\"HUGGINGFACE_MODEL_NAME_\" + p_model, None)\n",
    "    api_key = config.get(\"HUGGINGFACE_API_KEY\", None)\n",
    "    api_url = config.get(\"HUGGINGFACE_API_URL_\" + p_model, None)\n",
    "    return model_name, api_key, api_url\n",
    "\n",
    "def read_data_schema_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the data schema from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file containing the data schema.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a string.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_schema = file.read()\n",
    "    return data_schema\n",
    "\n",
    "def parse_text_between_tags(text, start_tag, end_tag):\n",
    "    \"\"\"\n",
    "    Parses the text between the specified start and end tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search within.\n",
    "        start_tag (str): The start tag to look for.\n",
    "        end_tag (str): The end tag to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of matches found between the start and end tags.\n",
    "    \"\"\"\n",
    "    pattern = rf\"{re.escape(start_tag)}(.*?){re.escape(end_tag)}\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def run_sql_query(query):\n",
    "    \"\"\"\n",
    "    Executes the given SQL query and returns the result as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The SQL query to be executed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The result of the SQL query as a DataFrame.\n",
    "    \"\"\"\n",
    "    db_conn = os.getenv(\"DB_CONNECTION_STRING\")\n",
    "    conn = sqlite3.connect(db_conn)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await GenerateQuestions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ReadQuestionsAndGenerateAnswers(filename=\"questions/2024-06-23.txt\", selectedService=Service.HuggingFace, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace API Request: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct/v1/chat/completions\n",
      "HuggingFace API Request: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct/v1/chat/completions\n",
      "result String: <sql>\n",
      "SELECT \n",
      "  ROUND(AVG(list_price * (total_order - total_discount)),2) as Revenue,\n",
      "  ROUND(AVG(quantity),2) as Sales_Volume,\n",
      "  category_name\n",
      "FROM (\n",
      "  SELECT \n",
      "    list_price,\n",
      "    quantity,\n",
      "    total_discount,\n",
      "    DATEDIFF(required_date, order_date) as total_order,\n",
      "    DATEDIFF(required_date, order_date) * list_price - list_price * (total_order - total_discount) as total_discount,\n",
      "    category_id\n",
      "  FROM (\n",
      "    SELECT \n",
      "      p.list_price,\n",
      "      oi.quantity,\n",
      "      (oi.list_price - oi.discount) * oi.quantity as total_discount,\n",
      "      p.category_id\n",
      "    FROM products p\n",
      "    JOIN order_items oi ON p.product_id = oi.product_id\n",
      "  )\n",
      ")\n",
      "JOIN categories c ON category_id = c.category_id\n",
      "GROUP BY category_name\n",
      "ORDER BY Sales_Volume DESC\n",
      "LIMIT 5;\n",
      "</sql>\n",
      "\n",
      "<python>\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = None\n",
      "\n",
      "# Example data to create df\n",
      "df_categories = pd.DataFrame({\n",
      "  'category_id': [1, 2, 3],\n",
      "  'category_name': ['Children Bicycles', 'Comfort Bicycles', 'Road Bicycles']\n",
      "})\n",
      "\n",
      "df_products = pd.DataFrame({\n",
      "  'product_id': [10, 20, 30],\n",
      "  'product_name': ['Trek 820 - 2016', 'Ritchey Timberwolf', 'Surly Wednesday Frames'],\n",
      "  'brand_id': [1, 2, 3],\n",
      "  'category_id': [1, 2, 3],\n",
      " 'model_year': [2000, 2001, 2002],\n",
      "  'list_price': [100, 150, 200]\n",
      "})\n",
      "\n",
      "df_order_items = pd.DataFrame({\n",
      "  'order_id': [1, 1, 1, 2, 2, 2],\n",
      "  'product_id': [10, 20, 30, 10, 20, 30],\n",
      "  'quantity': [100, 50, 75, 50, 50, 75]\n",
      "})\n",
      "\n",
      "df_order_items_total = df_order_items.groupby('product_id')['quantity'].sum().reset_index()\n",
      "\n",
      "df_products_revenue = pd.merge(df_products, df_order_items_total, on='product_id')\n",
      "df_products_revenue['total_discount'] = df_products_revenue['list_price\n",
      "User query: most sold products\n",
      "Rephrased prompt: Based on the provided user input, a more specific and clear prompt can be generated as follows:\n",
      "\n",
      "\"Visualize the top-selling products across all categories, including their sales volume and revenue. Please display this data in a bar chart with the product category as the x-axis and sales volume as the y-axis. Additionally, include a table that lists the top 5 best-selling products, along with their sales volume and revenue.\"\n",
      "\n",
      "Recommended chart or visualization: \n",
      "\n",
      "Bar Chart with Product Category on x-axis and Sales Volume on y-axis\n",
      "\n",
      "| Product Category  | Sales Volume  |\n",
      "| --- | ---      |\n",
      "| Children Bicycles | 1000 units  |\n",
      "| Comfort Bicycles  | 800 units   |\n",
      "| Road Bicycles     | 600 units   |\n",
      "\n",
      "Table with Top 5 Best-Selling Products:\n",
      "\n",
      "| Product Name          | Sales Volume  | Revenue     |\n",
      "| ---                    | ---          | ---         |\n",
      "| Trek 820 - 2016        | 250 units    | $95,000    |\n",
      "| Ritchey Timberwolf     | 200 units    | $80,000    |\n",
      "| Surly Wednesday Frames | 180 units    | $72,000    |\n",
      "| Electra Cruiser        | 150 units    | $60,000    |\n",
      "| Giant TCX               | 120 units    | $48,000    |#\n",
      "SQL:  \n",
      "SELECT \n",
      "  ROUND(AVG(list_price * (total_order - total_discount)),2) as Revenue,\n",
      "  ROUND(AVG(quantity),2) as Sales_Volume,\n",
      "  category_name\n",
      "FROM (\n",
      "  SELECT \n",
      "    list_price,\n",
      "    quantity,\n",
      "    total_discount,\n",
      "    DATEDIFF(required_date, order_date) as total_order,\n",
      "    DATEDIFF(required_date, order_date) * list_price - list_price * (total_order - total_discount) as total_discount,\n",
      "    category_id\n",
      "  FROM (\n",
      "    SELECT \n",
      "      p.list_price,\n",
      "      oi.quantity,\n",
      "      (oi.list_price - oi.discount) * oi.quantity as total_discount,\n",
      "      p.category_id\n",
      "    FROM products p\n",
      "    JOIN order_items oi ON p.product_id = oi.product_id\n",
      "  )\n",
      ")\n",
      "JOIN categories c ON category_id = c.category_id\n",
      "GROUP BY category_name\n",
      "ORDER BY Sales_Volume DESC\n",
      "LIMIT 5;\n",
      "\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT \n  ROUND(AVG(list_price * (total_order - total_discount)),2) as Revenue,\n  ROUND(AVG(quantity),2) as Sales_Volume,\n  category_name\nFROM (\n  SELECT \n    list_price,\n    quantity,\n    total_discount,\n    DATEDIFF(required_date, order_date) as total_order,\n    DATEDIFF(required_date, order_date) * list_price - list_price * (total_order - total_discount) as total_discount,\n    category_id\n  FROM (\n    SELECT \n      p.list_price,\n      oi.quantity,\n      (oi.list_price - oi.discount) * oi.quantity as total_discount,\n      p.category_id\n    FROM products p\n    JOIN order_items oi ON p.product_id = oi.product_id\n  )\n)\nJOIN categories c ON category_id = c.category_id\nGROUP BY category_name\nORDER BY Sales_Volume DESC\nLIMIT 5;\n': no such column: required_date",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\AR002288\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2262\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2262\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such column: required_date",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m PromptToQueryResult(prompt_rephrase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ,selectedService\u001b[38;5;241m=\u001b[39mService\u001b[38;5;241m.\u001b[39mHuggingFace)\n",
      "Cell \u001b[1;32mIn[122], line 306\u001b[0m, in \u001b[0;36mPromptToQueryResult\u001b[1;34m(debug, prompt_rephrase, selectedService, query, outputFileDir)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sql)\n\u001b[1;32m--> 306\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m matches_python \u001b[38;5;241m=\u001b[39m parse_text_between_tags(result_string,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<python>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</python>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches_python) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[122], line 392\u001b[0m, in \u001b[0;36mrun_sql_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m    390\u001b[0m db_conn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB_CONNECTION_STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(db_conn)\n\u001b[1;32m--> 392\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\AR002288\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:486\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AR002288\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2326\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2317\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2324\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2326\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\AR002288\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2274\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2274\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT \n  ROUND(AVG(list_price * (total_order - total_discount)),2) as Revenue,\n  ROUND(AVG(quantity),2) as Sales_Volume,\n  category_name\nFROM (\n  SELECT \n    list_price,\n    quantity,\n    total_discount,\n    DATEDIFF(required_date, order_date) as total_order,\n    DATEDIFF(required_date, order_date) * list_price - list_price * (total_order - total_discount) as total_discount,\n    category_id\n  FROM (\n    SELECT \n      p.list_price,\n      oi.quantity,\n      (oi.list_price - oi.discount) * oi.quantity as total_discount,\n      p.category_id\n    FROM products p\n    JOIN order_items oi ON p.product_id = oi.product_id\n  )\n)\nJOIN categories c ON category_id = c.category_id\nGROUP BY category_name\nORDER BY Sales_Volume DESC\nLIMIT 5;\n': no such column: required_date"
     ]
    }
   ],
   "source": [
    "await PromptToQueryResult(prompt_rephrase=True, debug=True ,selectedService=Service.HuggingFace)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
