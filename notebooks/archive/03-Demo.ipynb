{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and AI Democratization through PromptToQueryResult Function\n",
    "\n",
    "In the context of data and AI democratization, `PromptToQueryResult` could be a function that translates natural language prompts into database queries. This could be part of a larger system that allows non-technical users to interact with databases using natural language, which would democratize access to data and AI.\n",
    "\n",
    "## Business Value and Domain Problem Solutions\n",
    "\n",
    "### Business Value:\n",
    "\n",
    "- **Increased Efficiency:** Non-technical users can retrieve data without needing to learn SQL or other query languages, or without needing to wait for technical staff to retrieve the data for them.\n",
    "- **Reduced Costs:** Less time and resources are spent on training staff to use complex database systems, and less time is spent on data retrieval tasks by technical staff.\n",
    "- **Improved Decision Making:** With easier access to data, decision-making can be data-driven and timely, leading to better business outcomes.\n",
    "\n",
    "### Domain Problem Solved:\n",
    "\n",
    "- **Data Accessibility:** One of the challenges in data democratization is making data accessible to non-technical users. `PromptToQueryResult` could help solve this problem by providing a natural language interface to databases.\n",
    "- **AI Democratization:** By using AI to translate natural language prompts into database queries, `PromptToQueryResult` could also contribute to AI democratization. It could be part of a system that allows users to leverage the power of AI without needing to understand the technical details.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "The selected code is written in Python and it's part of a larger program that uses a plugin-based architecture. The code is using a kernel object to manage plugins and their functions.\n",
    "\n",
    "- **Variable Definitions:** First, two variables are defined: `plugins_directory` and `file_path`. The `plugins_directory` variable is set to the string `\"plugins\"`, which is presumably the directory where the plugins are stored. The `file_path` variable is set to `\"data_schema.txt\"`, which is likely a file that contains a data schema.\n",
    "\n",
    "- **Reading Data Schema:** The `read_data_schema_from_file` function is then called with `file_path` as an argument. This function reads the data schema from the specified file and returns it as a string. The returned data schema is stored in the `data_schema` variable.\n",
    "\n",
    "- **Conditional Plugin Import:** Next, there's an if-statement that checks the `prompt_rephrase` variable. If `prompt_rephrase` is true, the code imports a plugin named `\"PromptPlugin\"` from the plugins directory using the `import_plugin_from_prompt_directory` method of the kernel object. This method returns a dictionary-like object of functions provided by the plugin. The `\"PromptRephraser\"` function from the `\"PromptPlugin\"` plugin is then stored in the `rephraserFunction` variable.\n",
    "\n",
    "- **Function Invocation:** The `rephraserFunction` is then invoked asynchronously using the `invoke` method of the kernel object. The `invoke` method is called with two arguments: the function to be invoked and a `KernelArguments` object that contains the data schema and a query. The result of the function invocation is stored in the `rephrased_prompt` variable.\n",
    "\n",
    "- **Importing Another Plugin:** Finally, the code imports another plugin named `\"DataPlugin\"` from the plugins directory and stores the `\"DatabaseDescriptor\"` function from this plugin in the `descriptorFunction` variable. This function can presumably be used later in the code to describe a database based on the data schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from services import Service\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import dotenv_values\n",
    "import semantic_kernel.connectors.ai.hugging_face as sk_hf\n",
    "import datetime\n",
    "import json\n",
    "from huggingface_hub import InferenceApi\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from pydantic import BaseModel  # Import Pydantic BaseModel\n",
    "import requests  # Import requests library\n",
    "\n",
    "async def GenerateQuestions(selectedService=Service.AzureOpenAI,huggingface_model=\"Llama318BInstruct\"):\n",
    "    \"\"\"\n",
    "    Generates possible questions using the Semantic Kernel and prints the results.\n",
    "    \"\"\"\n",
    "    kernel = sk.Kernel()\n",
    "    service_id = None\n",
    "    if selectedService == Service.OpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "        api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "        service_id = \"gpt4-32k\"\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(service_id=service_id, ai_model_id=\"gpt4-32k\", api_key=api_key, org_id=org_id),\n",
    "        )\n",
    "    elif selectedService == Service.AzureOpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "        deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "        service_id = \"aoai_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key),\n",
    "        )\n",
    "    elif selectedService == Service.HuggingFace:\n",
    "        from huggingface_hub import InferenceApi\n",
    "        model_id, api_token, api_url = hugging_face_settings_from_dot_env(huggingface_model)\n",
    "        # api_token = \"hf_...\"  # Set your HuggingFace API token here\n",
    "        # model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # Set the model ID you want to use from HuggingFace\n",
    "        # api_url = f\"https://api-inference.huggingface.co/models/{model_id}/v1/chat/completions\"\n",
    "        # Initialize the HuggingFace Inference API without task override\n",
    "        hf_service = InferenceApi(repo_id=model_id, token=api_token)\n",
    "    \n",
    "\n",
    "       # Define a simple completion object to hold both the response and metadata\n",
    "        class CompletionResult:\n",
    "            def __init__(self, content: str, metadata: dict = None):\n",
    "                self.content = content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "            def __iter__(self):\n",
    "                return iter([self.content])  # Ensures CompletionResult can be iterated over\n",
    "\n",
    "        # Create the HuggingFaceChatCompletion class with a custom request\n",
    "        class HuggingFaceChatCompletion(ChatCompletionClientBase, BaseModel):\n",
    "            service_id: str\n",
    "            api_token: str\n",
    "            api_url: str\n",
    "            \n",
    "            class Config:\n",
    "                arbitrary_types_allowed = True\n",
    "\n",
    "            async def complete_async(self, messages: list, max_tokens: int = 500, stream: bool = False, **kwargs):\n",
    "                # Create the headers and payload for the request\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {self.api_token}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                }\n",
    "\n",
    "                payload = {\n",
    "                    \"model\": self.api_url.split('/')[-4],  # model_id\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"stream\": stream\n",
    "                }\n",
    "\n",
    "                # Make the request to HuggingFace API\n",
    "                response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "                response_json = response.json()\n",
    "\n",
    "                # Log the response for debugging\n",
    "                # print(\"HuggingFace API Response:\", response_json)\n",
    "                \n",
    "                # Handle the response format\n",
    "                if \"choices\" in response_json:\n",
    "                    content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    # Return a list of CompletionResult objects, even if there's only one\n",
    "                    return [CompletionResult(content=content, metadata={\"model\": self.ai_model_id})]\n",
    "                    #return content\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected response format: {response_json}\")\n",
    "\n",
    "            # Updated complete_chat to handle chat_history and settings\n",
    "            async def complete_chat(self, chat_history, settings, **kwargs):\n",
    "                messages = [{\"role\": \"user\", \"content\": msg.content} for msg in chat_history]\n",
    "                return await self.complete_async(messages)\n",
    "            \n",
    "            # Implement required complete_chat_stream method\n",
    "            async def complete_chat_stream(self, chat_history, settings, **kwargs):\n",
    "                # For non-streaming models, we can return the entire result as one chunk\n",
    "                result = await self.complete_chat(chat_history, settings)\n",
    "                for chunk in [result]:  # Mock streaming by yielding the entire result as a single chunk\n",
    "                    yield chunk\n",
    "\n",
    "        service_id = \"hf_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            HuggingFaceChatCompletion(service_id=service_id, api_token=api_token, api_url=api_url, ai_model_id=model_id),\n",
    "        )\n",
    "    \n",
    "    plugins_directory = \"plugins\"\n",
    "    file_path = \"data_schema.txt\"\n",
    "    data_schema = read_data_schema_from_file(file_path)\n",
    "    promptFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"DataPlugin\")\n",
    "    queryGeneratorFunction = promptFunctions[\"QuestionGenerator\"]\n",
    "    result = await kernel.invoke(queryGeneratorFunction, sk.KernelArguments(data_schema=data_schema))\n",
    "    if(hasattr(result, 'data')):\n",
    "        result_string = result.data\n",
    "    elif (result.__dict__):\n",
    "        # Access the value (which contains the list of CompletionResult objects)\n",
    "        # print(\"Value: \", result.value)\n",
    "        completion_results = result.value\n",
    "\n",
    "        # Check if it's a list and access the content of the first CompletionResult\n",
    "        if isinstance(completion_results, list) and len(completion_results) > 0:\n",
    "            first_result = completion_results[0]\n",
    "            # print(first_result.content)  # This will print the content of the first result\n",
    "            result_string = first_result.content\n",
    "        else:\n",
    "            print(\"No completion results found.\")\n",
    "    else:\n",
    "        result_string = str(result)\n",
    "    # Generate a unique filename based on the current date\n",
    "    filename = \"questions/\" + datetime.datetime.now().strftime(\"%Y-%m-%d\") + \".txt\"\n",
    "\n",
    "    # Write result_string to the file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(result_string)\n",
    "\n",
    "    # Print the filename\n",
    "    print(\"Result saved to:\", filename)\n",
    "    print(result_string)\n",
    "\n",
    "async def ReadQuestionsAndGenerateAnswers(filename, debug=False, selectedService=Service.AzureOpenAI):\n",
    "    \"\"\"\n",
    "    Reads questions from a file and generates answers using the Semantic Kernel.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file containing the questions.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Extract the filename from the full path\n",
    "    filename = os.path.basename(filename)\n",
    "\n",
    "    # Remove the file extension\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Create the directory path\n",
    "    directory_path = \"answers/\" + filename \n",
    "\n",
    "    print(\"Directory path: \", directory_path)   \n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "\n",
    "    data = json.loads(content)\n",
    "    i = 0\n",
    "    for item in data:\n",
    "        complexity = item[\"complexity\"]\n",
    "        queries = item[\"queries\"]\n",
    "        \n",
    "        for question in queries:\n",
    "            print(\"Index:\" , i)\n",
    "            print(\"Question: \", question)\n",
    "            print(\"Complexity: \", complexity)\n",
    "            questionFolderName = i.__str__() + \"_\" + complexity\n",
    "            if not os.path.exists(directory_path + \"/\"  + questionFolderName + \"/\"):\n",
    "                os.makedirs(directory_path + \"/\"  + questionFolderName + \"/\")\n",
    "\n",
    "            try:\n",
    "                await PromptToQueryResult(debug=debug, prompt_rephrase=False, selectedService=selectedService, query=question, outputFileDir=directory_path + \"/\" + questionFolderName + \"/\")\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                with open(directory_path + \"/\" + questionFolderName + \"/error.txt\", \"w\") as file:\n",
    "                    file.write(error_message)\n",
    "            i = i + 1\n",
    "\n",
    "def make_safe_folder_name(folder_name):\n",
    "    # Define a regular expression pattern to match invalid characters\n",
    "    # This example replaces anything that is not a letter, number, underscore, or hyphen with an underscore\n",
    "    safe_folder_name = re.sub(r'[^A-Za-z0-9_\\-]', '_', folder_name)\n",
    "    return safe_folder_name\n",
    "\n",
    "async def PromptToQueryResult(debug=False, prompt_rephrase=False, selectedService=Service.AzureOpenAI, query=None, outputFileDir=\"\", huggingface_model=\"Llama318BInstruct\", model_mode=\"chat\"):\n",
    "    \"\"\"\n",
    "    Prompts the user for a query, rephrases the prompt if required, and executes the query using the Semantic Kernel.\n",
    "\n",
    "    Args:\n",
    "        debug (bool, optional): If True, prints debug information. Defaults to False.\n",
    "        prompt_rephrase (bool, optional): If True, rephrases the prompt using a rephraser plugin. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame or any: The result of the executed query, or 'any' if no query is executed.\n",
    "    \"\"\"\n",
    "    kernel = sk.Kernel()\n",
    "    service_id = None\n",
    "    if selectedService == Service.OpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "        api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "        service_id = \"gpt4-32k\"\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(service_id=service_id, ai_model_id=\"gpt4-32k\", api_key=api_key, org_id=org_id),\n",
    "        )\n",
    "    elif selectedService == Service.AzureOpenAI:\n",
    "        from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "        deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "        service_id = \"aoai_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key),\n",
    "        )\n",
    "    elif selectedService == Service.HuggingFace:\n",
    "        from huggingface_hub import InferenceApi\n",
    "        model_id, api_token, api_url = hugging_face_settings_from_dot_env(huggingface_model)\n",
    "        # api_token = \"hf_...\"  # Set your HuggingFace API token here\n",
    "        # model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # Set the model ID you want to use from HuggingFace\n",
    "        # api_url = f\"https://api-inference.huggingface.co/models/{model_id}/v1/chat/completions\"\n",
    "        # Initialize the HuggingFace Inference API without task override\n",
    "        hf_service = InferenceApi(repo_id=model_id, token=api_token)\n",
    "    \n",
    "\n",
    "        # Define a simple completion object to hold both the response and metadata\n",
    "        class CompletionResult:\n",
    "            def __init__(self, content: str, metadata: dict = None):\n",
    "                self.content = content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "            def __iter__(self):\n",
    "                return iter([self.content])  # Ensures CompletionResult can be iterated over\n",
    "\n",
    "        # Create the HuggingFaceChatCompletion class with a custom request\n",
    "        class HuggingFaceChatCompletion(ChatCompletionClientBase, BaseModel):\n",
    "            service_id: str\n",
    "            api_token: str\n",
    "            api_url: str\n",
    "            mode: str = \"chat\"\n",
    "            \n",
    "            class Config:\n",
    "                arbitrary_types_allowed = True\n",
    "\n",
    "            async def complete_async(self, messages: list, max_tokens: int = 500, stream: bool = False, **kwargs):\n",
    "                # Create the headers and payload for the request\n",
    "                if(debug):\n",
    "                    print(\"HuggingFace API Request:\", messages)\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {self.api_token}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                }\n",
    "\n",
    "                if(self.mode == \"chat\"):\n",
    "                    payload = {\n",
    "                        \"model\": self.api_url.split('/')[-4],  # model_id\n",
    "                        \"messages\": messages,\n",
    "                        \"max_tokens\": max_tokens,\n",
    "                        \"stream\": stream\n",
    "                    }\n",
    "                else:\n",
    "                    payload = {\n",
    "                        \"inputs\": messages,  # Use 'inputs' field for text completion\n",
    "                        \"parameters\": {\"max_new_tokens\": max_tokens}\n",
    "                    }\n",
    "\n",
    "                # Make the request to HuggingFace API\n",
    "                response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "                response_json = response.json()\n",
    "\n",
    "                # Log the response for debugging\n",
    "                # print(\"HuggingFace API Response:\", response_json)\n",
    "                if(debug):\n",
    "                    print(\"HuggingFace API Response:\", response_json)\n",
    "                # Handle the response format\n",
    "                if \"choices\" in response_json:\n",
    "                    \n",
    "                    content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    # Return a list of CompletionResult objects, even if there's only one\n",
    "                    return [CompletionResult(content=content, metadata={\"model\": self.ai_model_id})]\n",
    "                    #return content\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected response format: {response_json}\")\n",
    "\n",
    "            # Updated complete_chat to handle chat_history and settings\n",
    "            async def complete_chat(self, chat_history, settings, **kwargs):\n",
    "                messages = [{\"role\": \"user\", \"content\": msg.content} for msg in chat_history]\n",
    "                return await self.complete_async(messages)\n",
    "            \n",
    "            # Implement required complete_chat_stream method\n",
    "            async def complete_chat_stream(self, chat_history, settings, **kwargs):\n",
    "                # For non-streaming models, we can return the entire result as one chunk\n",
    "                result = await self.complete_chat(chat_history, settings)\n",
    "                for chunk in [result]:  # Mock streaming by yielding the entire result as a single chunk\n",
    "                    yield chunk\n",
    "        service_id = \"hf_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            HuggingFaceChatCompletion(service_id=service_id, api_token=api_token, api_url=api_url, ai_model_id=model_id, mode=model_mode, max_tokens=2500),\n",
    "        )\n",
    "    elif selectedService == Service.ClaudeAI:\n",
    "        # Import the Anthropics library\n",
    "        import anthropic\n",
    "\n",
    "        # Define the Claude API settings (model and token)\n",
    "        claude_api_key, claude_model = claude_settings_from_dot_env()\n",
    "\n",
    "        # Define the CompletionResult class\n",
    "        class CompletionResult:\n",
    "            def __init__(self, content: str, metadata: dict = None):\n",
    "                self.content = content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "            def __iter__(self):\n",
    "                return iter([self.content])  # Ensures CompletionResult can be iterated over\n",
    "\n",
    "        # Create the ClaudeChatCompletion class\n",
    "        class ClaudeChatCompletion(ChatCompletionClientBase, BaseModel):\n",
    "            service_id: str\n",
    "            api_key: str\n",
    "            model: str = \"claude-3-5-sonnet-20241022\"  # Example model\n",
    "\n",
    "            class Config:\n",
    "                arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "\n",
    "            async def complete_async(self, messages: list, max_tokens: int = 500, temperature: float = 0.0, **kwargs):\n",
    "                if debug:\n",
    "                    print(\"Claude API Request:\", messages)\n",
    "                print(\"API Key:\", self.api_key)   \n",
    "                print(\"Model:\", self.model)\n",
    "                # Initialize the Anthropics API client\n",
    "                client = anthropic.Client(api_key=self.api_key)\n",
    "                \n",
    "                # Prepare the payload for the request\n",
    "                response = client.messages.create(\n",
    "                    model=self.model,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    system=\"You are an expert software assistant specializing in generating SQL and Python code. When given a natural language query, respond with optimized, well-structured, and efficient code only. Provide explanations only if explicitly requested, focusing primarily on generating accurate SQL and Python snippets that solve the query effectively.\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": msg[\"content\"]\n",
    "                                }\n",
    "                            ]\n",
    "                        } for msg in messages\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Check if response has content and that it's in the expected format\n",
    "                if hasattr(response, \"content\") and isinstance(response.content, list):\n",
    "                    # Extract the text from each TextBlock in the content list\n",
    "                    content = \" \".join([block.text for block in response.content if hasattr(block, \"text\")])\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected response format\")\n",
    "\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"Claude API Response:\", content)\n",
    "                \n",
    "                # Yield the single completion result\n",
    "                return CompletionResult(content=content, metadata={\"model\": self.model})\n",
    "\n",
    "            async def complete_chat(self, chat_history, settings, **kwargs):\n",
    "                max_tokens = getattr(settings, 'max_tokens', 500)\n",
    "                temperature = getattr(settings, 'temperature', 0.0)\n",
    "                messages = [{\"role\": \"user\", \"content\": msg.content} for msg in chat_history]\n",
    "                \n",
    "                # Await the result from complete_async\n",
    "                result = await self.complete_async(messages, max_tokens=max_tokens, temperature=temperature)\n",
    "                return [result]  # Return a list of CompletionResult objects\n",
    "\n",
    "            async def complete_chat_stream(self, chat_history, settings, **kwargs):\n",
    "                max_tokens = getattr(settings, 'max_tokens', 500)\n",
    "                temperature = getattr(settings, 'temperature', 0.0)\n",
    "                messages = [{\"role\": \"user\", \"content\": msg.content} for msg in chat_history]\n",
    "                \n",
    "                # Await the result from complete_async\n",
    "                result = await self.complete_async(messages, max_tokens=max_tokens, temperature=temperature)\n",
    "                \n",
    "                # Mock streaming by yielding each result in a single chunk\n",
    "                yield result\n",
    "\n",
    "\n",
    "        # Initialize the ClaudeChatCompletion service and add it to the kernel\n",
    "        service_id = \"claude_chat_completion\"\n",
    "        kernel.add_service(\n",
    "            ClaudeChatCompletion(service_id=service_id, api_key=claude_api_key, ai_model_id=claude_model, model=claude_model),\n",
    "        )\n",
    "        \n",
    "\n",
    "    \n",
    "    if(query is None):\n",
    "        query = input(\"Enter your query: \")  # Get query from user\n",
    "    rephrased_prompt = query\n",
    "    \n",
    "    plugins_directory = \"plugins\"\n",
    "    file_path = \"data_schema.txt\"\n",
    "    data_schema = read_data_schema_from_file(file_path)\n",
    "    if(prompt_rephrase):\n",
    "        promptFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"PromptPlugin\")\n",
    "        rephraserFunction = promptFunctions[\"PromptRephraser\"]\n",
    "        rephrased_prompt_result = await kernel.invoke(rephraserFunction, sk.KernelArguments(data_schema=data_schema, query=query))\n",
    "        # Debug: Print the type and value of the result to understand its structure\n",
    "        # print(type(rephrased_prompt_result))  # This will show whether it's a tuple, list, or other type\n",
    "        # print(rephrased_prompt_result)        # This will print the raw result\n",
    "        # print(\"Kernel invoke result:\", rephrased_prompt_result.__dict__)\n",
    "        if(hasattr(rephrased_prompt_result, 'data')):\n",
    "            rephrased_prompt = rephrased_prompt_result.data\n",
    "        elif (rephrased_prompt_result.__dict__):\n",
    "            # Access the value (which contains the list of CompletionResult objects)\n",
    "            # print(\"Value: \", rephrased_prompt_result.value)\n",
    "            completion_results = rephrased_prompt_result.value\n",
    "\n",
    "            # Check if it's a list and access the content of the first CompletionResult\n",
    "            if isinstance(completion_results, list) and len(completion_results) > 0:\n",
    "                first_result = completion_results[0]\n",
    "                # print(first_result.content)  # This will print the content of the first result\n",
    "                rephrased_prompt = first_result.content\n",
    "            else:\n",
    "                print(\"No completion results found.\")\n",
    "        else:\n",
    "            rephrased_prompt = str(rephrased_prompt_result)\n",
    "        #rephrased_prompt = rephrased_prompt_result.data if hasattr(rephrased_prompt_result, 'data') else str(rephrased_prompt_result)\n",
    "\n",
    "    dataFunctions = kernel.import_plugin_from_prompt_directory(plugins_directory, \"DataPlugin\")\n",
    "    descriptorFunction = dataFunctions[\"DatabaseDescriptor\"]\n",
    "\n",
    "    savePlotToDisk = \"\"\n",
    "    if(outputFileDir != \"\"):\n",
    "        savePlotToDisk = \"Generated plots should be saved in the directory: \" + outputFileDir + \"plot.png\"\n",
    "\n",
    "    if(outputFileDir != \"\"):\n",
    "        # Write rephrased prompt to query.txt file\n",
    "        with open(outputFileDir + \"user_prompt.txt\", \"w\") as file:\n",
    "            file.write(rephrased_prompt)\n",
    "\n",
    "    result = await kernel.invoke(descriptorFunction, sk.KernelArguments(data_schema=data_schema, query= rephrased_prompt, save_plot_to_disk = savePlotToDisk))\n",
    "    if(hasattr(result, 'data')):\n",
    "        result_string = result.data\n",
    "    elif (result.__dict__):\n",
    "        # Access the value (which contains the list of CompletionResult objects)\n",
    "        # print(\"Value: \", result.value)\n",
    "        completion_results = result.value\n",
    "\n",
    "        # Check if it's a list and access the content of the first CompletionResult\n",
    "        if isinstance(completion_results, list) and len(completion_results) > 0:\n",
    "            first_result = completion_results[0]\n",
    "            # print(first_result.content)  # This will print the content of the first result\n",
    "            result_string = first_result.content\n",
    "        else:\n",
    "            print(\"No completion results found.\")\n",
    "    else:\n",
    "        result_string = str(result)\n",
    "    if(debug):\n",
    "        print(\"result String:\", result_string)\n",
    "    matches_sql = parse_text_between_tags(result_string,\"<sql>\", \"</sql>\")\n",
    "\n",
    "    if(prompt_rephrase):\n",
    "        print(\"User query: \" + query)\n",
    "    print(\"Rephrased prompt: \" + rephrased_prompt + \"#\")\n",
    "    if len(matches_sql) > 0:\n",
    "        sql = matches_sql[0]\n",
    "        if(outputFileDir != \"\"):\n",
    "            # Write query to .txt file\n",
    "            with open(outputFileDir + \"sql_query.txt\", \"w\") as file:\n",
    "                file.write(sql)\n",
    "        if debug:\n",
    "            print(\"SQL: \", sql)\n",
    "        df = run_sql_query(sql)\n",
    "    \n",
    "    \n",
    "\n",
    "    matches_python = parse_text_between_tags(result_string,\"<python>\", \"</python>\")\n",
    "    if len(matches_python) > 0:\n",
    "        if debug:\n",
    "            print(\"PYTHON:\", matches_python[0])\n",
    "        try:\n",
    "            db_conn = os.getenv(\"DB_CONNECTION_STRING\")\n",
    "            conn = sqlite3.connect(db_conn)\n",
    "            exec(matches_python[0])\n",
    "            conn.close()\n",
    "\n",
    "            if(outputFileDir != \"\"):\n",
    "                # Write python code to .txt file\n",
    "                with open(outputFileDir + \"python_code.txt\", \"w\") as file:\n",
    "                    file.write(matches_python[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print('hata:' + e)\n",
    "        except:\n",
    "            print(\"An exception occurred\")\n",
    "    if len(matches_sql) > 0:\n",
    "        df.head()\n",
    "        if(outputFileDir != \"\"):\n",
    "            df.to_csv(outputFileDir + \"output.csv\", index=False)\n",
    "        return df\n",
    "    else:  \n",
    "        return any\n",
    "    \n",
    "def claude_settings_from_dot_env():\n",
    "    \"\"\"\n",
    "    Reads the Claude API settings from the .env file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the model ID, API token, and API URL.\n",
    "\n",
    "    \"\"\"\n",
    "    config = dotenv_values(\".env\")\n",
    "    model_id = config.get(\"CLAUDE_MODEL_ID\", None)\n",
    "    api_token = config.get(\"CLAUDE_API_KEY\", None)\n",
    "    return api_token, model_id\n",
    "\n",
    "def hugging_face_settings_from_dot_env(p_model=\"Llama318BInstruct\") :\n",
    "\n",
    "    config = dotenv_values(\".env\")\n",
    "    model_name = config.get(\"HUGGINGFACE_MODEL_NAME_\" + p_model, None)\n",
    "    api_key = config.get(\"HUGGINGFACE_API_KEY\", None)\n",
    "    api_url = config.get(\"HUGGINGFACE_API_URL_\" + p_model, None)\n",
    "    return model_name, api_key, api_url\n",
    "\n",
    "def read_data_schema_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the data schema from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file containing the data schema.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a string.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_schema = file.read()\n",
    "    return data_schema\n",
    "\n",
    "def parse_text_between_tags(text, start_tag, end_tag):\n",
    "    \"\"\"\n",
    "    Parses the text between the specified start and end tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search within.\n",
    "        start_tag (str): The start tag to look for.\n",
    "        end_tag (str): The end tag to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of matches found between the start and end tags.\n",
    "    \"\"\"\n",
    "    pattern = rf\"{re.escape(start_tag)}(.*?){re.escape(end_tag)}\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def run_sql_query(query):\n",
    "    \"\"\"\n",
    "    Executes the given SQL query and returns the result as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The SQL query to be executed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The result of the SQL query as a DataFrame.\n",
    "    \"\"\"\n",
    "    db_conn = os.getenv(\"DB_CONNECTION_STRING\")\n",
    "    conn = sqlite3.connect(db_conn)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await GenerateQuestions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ReadQuestionsAndGenerateAnswers(filename=\"questions/2024-06-23.txt\", selectedService=Service.HuggingFace, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await PromptToQueryResult(prompt_rephrase=True, debug=True ,selectedService=Service.ClaudeAI, huggingface_model=\"Llama318BInstruct\", model_mode=\"chat\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
